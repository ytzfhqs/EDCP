论文原址：[How does the pre-training objective affect what large language models learn about linguistic properties?](https://aclanthology.org/2022.acl-short.16/)

本文探讨了不同预训练目标对大型语言模型（如BERT）学习语言属性的影响。研究者们使用了两种受语言学启发的预训练目标和三种非语言学启发的目标来预训练BERT模型，并随后对模型进行了语言特性编码的探测测试。实验结果表明，不同类型的预训练目标在探测性能上几乎没有差异。这一发现对主流观点——即语言学启发的预训练目标更为有效——提出了质疑。

## 方法
### 预训练

- 语言驱动目标

1. 掩码语言建模（MLM）
   随机选择输入句子中的15%的token，其中80%替换为[MASK]标记，10%替换为随机标记，剩下的10%保持不变。

2. 操纵词检测（S+R）
   选择输入序列中的10%的token并用打乱顺序的token替换，同时选择另外10%的token用词汇中的随机token替换。

- 非语言学启发目标

   - 掩码首字符预测（First Char）
     模型只预测掩码标记的第一个字符，例如'[c]at'和'[c]omputer'属于同一类。

   - 掩码ASCII码求和预测（ASCII）
     模型需预测掩码标记中字符的ASCII值之和，并将其取模5得到类别。

   - 掩码随机标记分类（Random）
     完全随机的目标，15%的输入标记被掩码，并随机指定每个掩码标记一个从0到4的类别，进行五分类任务。

# 探测任务
1. 表面信息任务：SentLen任务旨在正确预测句子中的单词数目。
2. 句法信息任务：TreeDepth测试表示是否保留了句子的层次结构信息，TopConst预测句子的顶级成分，BShift测试是否检测到相邻词序颠倒。
3. 语义信息任务：Tense任务预测主句动词是现在时还是过去时，SubjNum任务预测句子的主语数（单数/复数），ObjNum任务预测宾语数，SOMO任务检测是否出现角色扮演错误，CoordInv任务检测是否存在协调反转。
# 结果
## 探测任务表现
- 表面信息：SentLen任务表现良好。
- 句法信息：TreeDepth、TopConst和BShift任务表现各异。
- 语义信息：Tense、SubjNum、ObjNum、SOMO和CoordInv任务表现各异。
-  细分层表现
  - 在不同层的Bigram Shift（BShift）探测任务中，MLM、S+R、First Char、ASCII和Random目标的性能有所变化。
  - 在Top Constituent（TopConst）探测任务中，不同层的表现也各不相同。

# 讨论
研究结果暗示，即使是直观上对人类来说难以猜测关联性的非语言动机强的目标，也可能不会显著影响BERT学习语言特性的方式。这一发现对当前语言模型训练的策略提出了新的思考，表明可能需要重新审视预训练目标的设计。

# 结论
本文通过对BERT模型使用不同预训练目标的实验，发现语言动机强的目标与非语言动机强的目标之间在探测性能上的差异并不大。这一结果挑战了当前关于语言信息预训练效果的主流观点，并提示研究者应进一步探索预训练目标的设计，以优化语言模型的学习。

